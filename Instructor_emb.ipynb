{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "751835a3-2002-462a-b812-f34022afdee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "import torch\n",
    "import collections\n",
    "import imodelsx\n",
    "import string\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfd9c99-ba59-46d8-964e-2d44f06f41f5",
   "metadata": {},
   "source": [
    "## EDA of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "30929b75-bcdc-4e70-a3db-e6c10f3925af",
   "metadata": {},
   "outputs": [],
   "source": [
    "sst2 = datasets.load_dataset('sst2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "53d96240-37e6-4e3e-92b0-de4b2fa39480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': ['idx', 'sentence', 'label'],\n",
       " 'validation': ['idx', 'sentence', 'label'],\n",
       " 'test': ['idx', 'sentence', 'label']}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dir(sst2)\n",
    "sst2.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "540b8eaa-28ed-4c85-8c98-8fd94f9dc674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 67349, 'validation': 872, 'test': 1821}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sst2.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7b9d47eb-c810-423c-a12a-090d3858fb14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hide new secretions from the parental units ',\n",
       " 'contains no wit , only labored gags ',\n",
       " 'that loves its characters and communicates something rather beautiful about human nature ',\n",
       " 'remains utterly satisfied to remain the same throughout ',\n",
       " 'on the worst revenge-of-the-nerds clichÃ©s the filmmakers could dredge up ',\n",
       " \"that 's far too tragic to merit such superficial treatment \",\n",
       " 'demonstrates that the director of such hollywood blockbusters as patriot games can still turn out a small , personal film with an emotional wallop . ',\n",
       " 'of saucy ',\n",
       " \"a depressed fifteen-year-old 's suicidal poetry \",\n",
       " \"are more deeply thought through than in most ` right-thinking ' films \"]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sample of first 10 reviews\n",
    "sst2['train']['sentence'][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6e08b9a3-4742-4aa5-b2ff-e10ca7553c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'contains no wit , only labored gags '"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sst2['train']['sentence'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "da9eab18-09eb-4f34-960f-8da15472c064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'uneasy mishmash of styles and genres .'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sst2['test']['sentence'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be7f73b-c3e6-474d-bb82-f52244fd3501",
   "metadata": {},
   "source": [
    "## INSTRUCTOR ðŸ‘¨â€ðŸ«ðŸ‘©â€ðŸ«"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f8733338-6e89-4e25-bd1a-70842cb6063d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "#dir(INSTRUCTOR)\n",
    "model = INSTRUCTOR('hkunlp/instructor-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5222b5fc-46ff-4f1a-9f83-74ec0b5f3ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.3520196e-02 -1.4751182e-02 -2.1812499e-02 ... -5.3077709e-02\n",
      "  -1.9894231e-02  4.6290603e-02]\n",
      " [-4.0275887e-02  9.5391721e-03 -2.0763531e-02 ... -5.9346016e-02\n",
      "   1.6357798e-02  3.0490218e-02]\n",
      " [-3.5560861e-02 -2.3208736e-03 -1.3518459e-02 ... -2.9144358e-02\n",
      "   2.5669584e-02  6.4498499e-02]\n",
      " ...\n",
      " [-2.8902860e-02  9.5078964e-03 -1.9643229e-02 ... -3.4858063e-02\n",
      "   3.1831473e-02  4.5890577e-02]\n",
      " [-3.0532386e-02 -3.9113317e-02 -1.6344437e-02 ... -3.8398307e-02\n",
      "   2.1053439e-03  3.6098324e-02]\n",
      " [-3.3510122e-02 -2.5493458e-03 -1.9426448e-02 ... -6.0843486e-02\n",
      "   8.7424909e-05  4.9884342e-02]]\n"
     ]
    }
   ],
   "source": [
    "#dir(model)\n",
    "#get the embedding\n",
    "sentence = sst2['train']['sentence'][0:10]\n",
    "#instruction = \"semantic\"\n",
    "embeddings = model.encode(sentence)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edcac4c-296f-4d1f-9f12-fe01ba5d9ac6",
   "metadata": {},
   "source": [
    "## Calculate Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "46b2ca0b-2b6d-480c-8a27-5a84bd6fda81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.791337   0.7545798  0.792951   0.76039135 0.7968184  0.76513994\n",
      "  0.77183414 0.7897898  0.7880728  0.77604973]\n",
      " [0.8359742  0.79563093 0.83904195 0.8281418  0.85586786 0.7920675\n",
      "  0.81078637 0.83478475 0.8178705  0.8116954 ]\n",
      " [0.8400682  0.81742257 0.80808955 0.7690075  0.83724487 0.821731\n",
      "  0.81950223 0.84481335 0.80259347 0.83987105]\n",
      " [0.8199604  0.7720649  0.85261905 0.74618435 0.7806753  0.7816093\n",
      "  0.7777296  0.83189476 0.79248047 0.8085593 ]\n",
      " [0.8210979  0.8378202  0.7874433  0.84053195 0.8511729  0.762864\n",
      "  0.82096875 0.7945789  0.79173076 0.79623735]\n",
      " [0.82384497 0.7801317  0.8040639  0.79008037 0.8473133  0.78129834\n",
      "  0.7880322  0.7888309  0.78303826 0.82346255]\n",
      " [0.8051164  0.8481436  0.79881895 0.7911173  0.81736356 0.7747912\n",
      "  0.8397882  0.82004267 0.76506215 0.81981665]\n",
      " [0.8505894  0.8004862  0.8175191  0.7937734  0.83820117 0.8168696\n",
      "  0.81293756 0.85264456 0.8296662  0.8161603 ]\n",
      " [0.8235054  0.7667987  0.79704994 0.77834255 0.8182266  0.78134567\n",
      "  0.78994125 0.8060272  0.78559744 0.8066112 ]\n",
      " [0.81426024 0.8517576  0.7928171  0.788291   0.8171367  0.7804682\n",
      "  0.8303825  0.8412075  0.7848222  0.8164028 ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "sentences_a = sst2['train']['sentence'][0:10]\n",
    "#print(sentences_a)\n",
    "sentences_b = sst2['train']['sentence'][10:20]\n",
    "embeddings_a = model.encode(sentences_a)\n",
    "embeddings_b = model.encode(sentences_b)\n",
    "similarities = cosine_similarity(embeddings_a,embeddings_b)\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c778718-5a6a-44de-a02b-429bb9b8bb6e",
   "metadata": {},
   "source": [
    "## modified code to take care of the feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c5f8ff4b-c1a7-4d96-9656-67e86a6c64a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "model = INSTRUCTOR('hkunlp/instructor-large')\n",
    "sentences_a = sst2['train']['sentence'][0:10]\n",
    "sentences_b = sst2['train']['sentence'][10:20]\n",
    "\n",
    "def extract_ngrams(sentence, n):\n",
    "    tokens = sentence.split()\n",
    "    ngrams_list = list(ngrams(tokens, n))\n",
    "    return [' '.join(gram) for gram in ngrams_list]\n",
    "\n",
    "similarities = []\n",
    "\n",
    "for sentence_a in sentences_a:\n",
    "    for sentence_b in sentences_b:\n",
    "        embeddings_a = model.encode(sentence_a)\n",
    "        embeddings_b = model.encode(sentence_b)\n",
    "        similarity = cosine_similarity(embeddings_a.reshape(1, -1), embeddings_b.reshape(1, -1))[0][0]\n",
    "        \n",
    "        # Perform n-gram averaging\n",
    "        n = 2  # You can adjust the n-gram size\n",
    "        ngrams_a = extract_ngrams(sentence_a, n)\n",
    "        ngrams_b = extract_ngrams(sentence_b, n)\n",
    "        \n",
    "        # Calculate the importance of common n-grams\n",
    "        common_ngrams = set(ngrams_a) & set(ngrams_b)\n",
    "        ngram_importance = {ngram: ngrams_a.count(ngram) + ngrams_b.count(ngram) for ngram in common_ngrams}\n",
    "        \n",
    "        similarities.append((similarity, ngram_importance))\n",
    "\n",
    "# Now you have a list of tuples containing similarity scores and n-gram importance dictionaries for each pair of sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "42b0abab-0464-4b53-b71a-fa33ba5db08a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentence_list:\n\u001b[1;32m      8\u001b[0m     texts \u001b[38;5;241m=\u001b[39m imodelsx\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mgenerate_ngrams_list(sentence, ngrams\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, all_ngrams\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 9\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m(texts, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     10\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\u001b[38;5;241m.\u001b[39mlast_hidden_state\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     11\u001b[0m     embs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(outputs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize the defaultdict\n",
    "d = collections.defaultdict(list)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # generate ngrams up to trigrams\n",
    "    for i, sentence_list in enumerate([sentences_a, sentences_b]):\n",
    "        for sentence in sentence_list:\n",
    "            texts = imodelsx.util.generate_ngrams_list(sentence, ngrams=3, all_ngrams=True)\n",
    "            inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            outputs = model(**inputs).last_hidden_state.detach().cpu().numpy()\n",
    "            embs = np.mean(outputs, axis=1).squeeze()\n",
    "            embs_mean = np.mean(embs, axis=0)\n",
    "\n",
    "            d['texts'].append(texts)\n",
    "            d['embs'].append(embs)\n",
    "            d['embs_mean'].append(embs_mean)\n",
    "\n",
    "    # calculate feature importance for similarity\n",
    "    denominator = calculate_denominator(d['embs_mean'][0], d['embs_mean'][1])\n",
    "    d['imps'].append((d['embs'][0] @ d['embs_mean'][1]) / denominator)\n",
    "    d['imps'].append((d['embs'][1] @ d['embs_mean'][0]) / denominator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9223e4a7-6579-48f7-ac72-e1e574bce4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "83799434-4120-4ce7-8af5-5a710b923988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MAXYEAR',\n",
       " 'MINYEAR',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " 'date',\n",
       " 'datetime',\n",
       " 'datetime_CAPI',\n",
       " 'sys',\n",
       " 'time',\n",
       " 'timedelta',\n",
       " 'timezone',\n",
       " 'tzinfo']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a35481f-fbf4-4f1b-9ca3-3d787a12081d",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime.datetime()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
